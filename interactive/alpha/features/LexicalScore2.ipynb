{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "nlp=spacy.load('en')\n",
    "# wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchlist(file):\n",
    "    filecontent=open(file)\n",
    "    lines=filecontent.readlines()\n",
    "    l=[]\n",
    "    for line in lines:\n",
    "        l.append(line.strip())\n",
    "    #print(list)\n",
    "    return l\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def customfind(sent,phrase):\n",
    "    pp=phrase.split(\" \")\n",
    "    if len(pp)>1:\n",
    "        if sent.find(phrase)!=-1:\n",
    "            return True;\n",
    "    else:\n",
    "        words=sent.split(\" \")\n",
    "        for word in words:\n",
    "            if(word==phrase):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(customfind(\"Hi kelly michaels you gone\",\"ell\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def badcount(document,badlist):\n",
    "    count=0\n",
    "    for phrase_ in badlist:\n",
    "        if customfind(document,phrase_):\n",
    "            #print(phrase_)\n",
    "            count=count+1\n",
    "    return [count, count*10/word_tokenize(document).__len__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def positivecount(document,goodlist):\n",
    "    count=0\n",
    "    for phrase_ in goodlist:\n",
    "        if customfind(document,phrase_):\n",
    "            #print(phrase_)\n",
    "            count=count+1\n",
    "    return [count,count*10/word_tokenize(document).__len__()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentencelenratio(sentence, shortis):\n",
    "    short_sent=0\n",
    "    sent_tokens=sent_tokenize(sentence)\n",
    "    for sent in sent_tokens:\n",
    "        if sent.__len__() <= shortis:\n",
    "            short_sent=short_sent+1\n",
    "    return short_sent/sent_tokens.__len__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capitalratio(document):\n",
    "    uc_token=0\n",
    "    w_tokens=word_tokenize(document)\n",
    "    words_analysed=0\n",
    "    #print(w_tokens)\n",
    "    for word in w_tokens :\n",
    "        #print(word)\n",
    "        if word.__len__()>1 and (word!=\"'s\" and word !=\"'S\" and word !=\"'ll\" and word !=\"'LL\" and word !=\"n't\" and word !=\"N'T\" and word !=\"'re\" and word !=\"'RE\" and word !=\"'d\" and word !=\"'D\" and word !=\"'ve\" and word !=\"'VE\"):\n",
    "                words_analysed=words_analysed+1\n",
    "                \n",
    "                if word.isupper():\n",
    "                    uc_token=uc_token +1\n",
    "                #print(word, words_analysed, uc_token)\n",
    "    val=uc_token/words_analysed\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OffensivenessScore(document,in_list,badlist,goodlist ):\n",
    "    s1=in_list[0]\n",
    "    s2=in_list[1]\n",
    "    s3=in_list[2]\n",
    "    s4=in_list[3]\n",
    "    s5=in_list[4]\n",
    "    s6=in_list[5]\n",
    "    s7=in_list[6]\n",
    "    s8=in_list[7]\n",
    "    s9=in_list[8]\n",
    "    s10=in_list[9]\n",
    "    s11=in_list[10]\n",
    "#s1 -for not in sibling -0\n",
    "#s2 -for you in sibling -1.5\n",
    "#s3 -for they in sibling -0.6\n",
    "#s4 -for 'nor, neither' in descendant -0\n",
    "#s5 -for you in descendant -1\n",
    "#s6 -for they in descendant -0.4\n",
    "#s7 -for you in parent's child -.6\n",
    "#s8 - for they in parent's child - .3\n",
    "#s9 - for otherwise -.2\n",
    "#s10 -for niece you - .8\n",
    "#s11 -for niece they - .45\n",
    "\n",
    "    wordlist=['yourself','your','yours','you','he','her','she','his','they','their','them','not','never','nobody','neither','nor','it','its']\n",
    "    document=document.lower()\n",
    "    doc = nlp(document)  \n",
    "    sentences_in_doc = doc.sents\n",
    "    #print(doc)\n",
    "    Flag=False\n",
    "    #displacy.serve(doc, style='dep')\n",
    "    score=0\n",
    "    token_no=0\n",
    "    for token in doc:\n",
    "        #check if token is a curse word\n",
    "        Flag=False\n",
    "        stemmedword=ps.stem(token.text)\n",
    "        \n",
    "        #lemma=wordnet_lemmatizer.lemmatize(token.text)\n",
    "        #print(\"token---\",token, \"stem---\",stemmedword, \"lemmatized---\",lemma)\n",
    "        if not token.is_stop and has(badlist,token.text):#(badlist.__contains__(token.text) or badlist.__contains__(stemmedword)) :#wordlist.__contains__(token.text) or ((not token.is_stop) and#and (token.head.pos_==VERB or token.head.pos_==NOUN or token.head.pos_==ADJ or token.head.pos_==ADV) and  IscurseWord(token.text):\n",
    "            #analysing 'sibling' relation\n",
    "            #print(\"ANALYSING--\",token)\n",
    "            if has(goodlist,doc[token_no+1].text):\n",
    "                Flag=True\n",
    "            if Flag:continue\n",
    "            for child in token.head.children:\n",
    "                if child.text=='not' or child.text=='never' or child.text=='nobody':  #child.text=='not' or child.text='never' or \n",
    "                    score=score+s1\n",
    "                    Flag=True\n",
    "                    break\n",
    "            if Flag:continue\n",
    "            for child in token.head.children:\n",
    "               \n",
    "                if child.text=='you'or child.text=='yourself' or child.text=='your' or child.text=='yours' or child.text=='it' or child.text=='its':\n",
    "                    score=score+s2\n",
    "                    #print(\"Rule--\",child)\n",
    "                    Flag=True\n",
    "                    break\n",
    "            if Flag:continue\n",
    "            for child in token.head.children:\n",
    "                if child.text=='they' or child.text=='he' or child.text=='she' or child.text=='her' or child.text=='their' or child.text=='them' or child.text=='his':\n",
    "                    score=score+s3\n",
    "                    Flag=True\n",
    "                    break\n",
    "            if Flag:continue\n",
    "            \n",
    "            #analysing 'descendant' relation\n",
    "            for node in token.children:\n",
    "                if node.text=='neither' or node.text=='nor': #node.text=='not' or node.text='never' or\n",
    "                    score=score+s4       #first descendant relation \n",
    "                    Flag=True\n",
    "                    break\n",
    "                else:\n",
    "                    for nodechild in node.children:\n",
    "                        if nodechild.text=='neither' or nodechild.text=='nor':\n",
    "                            score=score+(s4/2)     #grandchild descendant so score is halved\n",
    "                            Flag=True\n",
    "                            break              \n",
    "            if Flag:continue\n",
    "            for node in token.children:\n",
    "                if node.text=='you' or child.text=='yourself' or node.text=='your' or node.text=='yours' or node.text=='it' or node.text=='its':\n",
    "                    score=score+s5   \n",
    "                    #print(\"Rule--\",node)\n",
    "                    Flag=True\n",
    "                    break;\n",
    "                else:\n",
    "                    for nodechild in node.children:\n",
    "                        if nodechild.text=='you' or child.text=='yourself' or nodechild.text=='your' or nodechild.text=='yours' or nodechild.text=='it' or nodechild.text=='its':\n",
    "                            score=score+(s5/2)\n",
    "                            #print(\"Rule--\",node)\n",
    "                            Flag=True\n",
    "                            break  \n",
    "            if Flag:continue\n",
    "            for node in token.children:\n",
    "                if node.text=='they' or node.text=='he' or node.text=='she' or node.text=='her' or node.text=='their' or node.text=='them' or node.text=='his':\n",
    "                    score=score+s6\n",
    "                    Flag=True\n",
    "                    break;\n",
    "                else:\n",
    "                    for nodechild in node.children:\n",
    "                        if nodechild.text=='they' or nodechild.text=='he' or nodechild.text=='she' or nodechild.text=='her' or nodechild.text=='their' or nodechild.text=='them' or nodechild.text=='his':\n",
    "                            score=score+(s6/2)\n",
    "                            Flag=True\n",
    "                            break\n",
    "            if Flag:continue\n",
    "            #analysing niece relation\n",
    "            father=token.head\n",
    "            #print(\"pohcha\",father)\n",
    "            for child in father.children:\n",
    "                if child !=token:\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.text=='you' or child.text=='yourself' or grandchild.text=='your' or grandchild.text=='yours' or grandchild.text=='it' or grandchild.text=='its':\n",
    "                            score=score+s10\n",
    "                            #print(\"Rules--\",child)\n",
    "                            Flag=True\n",
    "                            break;\n",
    "            if Flag:continue\n",
    "            for child in father.children:\n",
    "                if child !=token:\n",
    "                    for grandchild in child.children:\n",
    "                        if grandchild.text=='they' or grandchild.text=='he' or grandchild.text=='she' or grandchild.text=='her' or grandchild.text=='their' or grandchild.text=='them' or grandchild.text=='his':\n",
    "                            score=score+s11\n",
    "                            #print(\"Rules--\",child)\n",
    "                            Flag=True\n",
    "                            break;\n",
    "            if Flag:continue\n",
    "            #analysing 'UNCLE' relation\n",
    "            if father.dep_!='ROOT':\n",
    "                for desc in father.head.children:\n",
    "                    if desc.text=='you' or child.text=='yourself' or desc.text=='your' or desc.text=='yours' or desc.text=='it' or desc.text=='its':\n",
    "                        score=score+s7\n",
    "                        #print(\"Rule--\",desc)\n",
    "                        Flag=True\n",
    "                        break;\n",
    "                if Flag:continue\n",
    "                for desc in father.head.children:\n",
    "                    if desc.text=='they' or desc.text=='he' or desc.text=='she' or desc.text=='her' or desc.text=='their' or desc.text=='them' or desc.text=='his':\n",
    "                        score=score+s8\n",
    "                        Flag=True\n",
    "                        break;\n",
    "            if Flag:\n",
    "                continue\n",
    "            else:\n",
    "                score=score+s9\n",
    "                #print(\"Rule--last\")\n",
    "        token_no=token_no+1\n",
    "    return score\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has(badlist,token):\n",
    "    for badword in badlist:\n",
    "        doc1=nlp(badword)\n",
    "        doc2=nlp(token)\n",
    "        if doc1.__len__() == 1 and doc2.__len__() ==1:\n",
    "            if doc1[0].text ==doc2[0].text or doc1[0].lemma_==doc2[0].lemma_:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list=['ghoul ghost', 'living curse', 'voices','tighter','kings']\n",
    "token=\"king\"\n",
    "print(has(list,token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexicalscore2(document,badlist):\n",
    "    nlp.vocab[\"you\"].is_stop = False\n",
    "    nlp.vocab[\"your\"].is_stop = False\n",
    "    nlp.vocab[\"yours\"].is_stop = False\n",
    "    nlp.vocab[\"yourself\"].is_stop = False\n",
    "    document=document.lower()  \n",
    "    sentences_l=sent_tokenize(document)\n",
    "    #doc=nlp(document)\n",
    "    lexscore=0\n",
    "    #sent_count=0\n",
    "    for sentence in sentences_l:\n",
    "        doc=nlp(sentence)\n",
    "        #print(doc)\n",
    "        badindexes=[]\n",
    "        youindexes=[]\n",
    "        for i in range (0,doc.__len__()):\n",
    "    #         token=doc[i].lemma_\n",
    "            #print(doc[i],\"---\",doc[i].is_stop)\n",
    "            if(not doc[i].is_stop):\n",
    "                if doc[i].text=='you' or doc[i].text=='your' or doc[i].text==\"your's\" or doc[i].text=='yours' or doc[i].text=='yourself':\n",
    "                    #print(i)\n",
    "                    youindexes.append(i)\n",
    "                    continue\n",
    "                if has(badlist,doc[i].text):#badlist.__contains__(token) or badlist.__contains__(token.lemma_):\n",
    "                    badindexes.append(i)\n",
    "                    continue\n",
    "        for badindex in badindexes:\n",
    "            for youindex in youindexes:\n",
    "                lexscore=lexscore+abs(badindex-youindex)   \n",
    "    nlp.vocab[\"you\"].is_stop = True\n",
    "    nlp.vocab[\"your\"].is_stop = True\n",
    "    nlp.vocab[\"yours\"].is_stop = True\n",
    "    nlp.vocab[\"yourself\"].is_stop = True\n",
    "    return lexscore/sentences_l.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexicalscore3(document,badlist):\n",
    "    nlp.vocab[\"you\"].is_stop = False\n",
    "    nlp.vocab[\"your\"].is_stop = False\n",
    "    nlp.vocab[\"yours\"].is_stop = False\n",
    "    nlp.vocab[\"yourself\"].is_stop = False\n",
    "    document=document.lower() \n",
    "#     sentence=nlp(document)\n",
    "    lexscore=0\n",
    "    doc=nlp(document)\n",
    "#     sent_count=0\n",
    "#         sent_count=sent_count+1\n",
    "     \n",
    "        #print(doc)\n",
    "    badindexes=[]\n",
    "    youindexes=[]\n",
    "    for i in range (0,doc.__len__()):\n",
    "    #         token=doc[i].lemma_\n",
    "            #print(doc[i],\"---\",doc[i].is_stop)\n",
    "        if(not doc[i].is_stop):\n",
    "            if doc[i].text=='you' or doc[i].text=='your' or doc[i].text==\"your's\" or doc[i].text=='yours' or doc[i].text=='yourself':\n",
    "                    #print(i)\n",
    "                youindexes.append(i)\n",
    "                continue\n",
    "            if has(badlist,doc[i].text):#badlist.__contains__(token) or badlist.__contains__(token.lemma_):\n",
    "                badindexes.append(i)\n",
    "                continue\n",
    "    for badindex in badindexes:\n",
    "        for youindex in youindexes:\n",
    "            lexscore=lexscore+abs(badindex-youindex)   \n",
    "    nlp.vocab[\"you\"].is_stop = True\n",
    "    nlp.vocab[\"your\"].is_stop = True\n",
    "    nlp.vocab[\"yours\"].is_stop = True\n",
    "    nlp.vocab[\"yourself\"].is_stop = True\n",
    "    return lexscore/doc.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#file='sortedBadWords_for_checking.txt'\n",
    "bad_list=fetchlist('sortedBadWords_for_checking.txt')\n",
    "good_list=fetchlist('positives.txt')\n",
    "youwords=['you','your']\n",
    "#print(bad_list.__contains__('lazy'),bad_list.__contains__('ass'))\n",
    "# sc=lexicalscore2(\"You are so lazy that it took you 11 months not to come out of your mother's ass\",bad_list)\n",
    "# sc1=lexicalscore2(\"Do not give those stupid explaination of yours\",bad_list) #non- processed data\n",
    "# sc2=lexicalscore2(\"you are a pig\",bad_list)\n",
    "# sc3=lexicalscore2(\"you are fucking awesome.\",bad_list)\n",
    "# print(sc,\"=\",round(sc,3))\n",
    "# print(sc1)\n",
    "# print(sc2)\n",
    "# print(sc3)\n",
    "# #---------------------------------------------------------------\n",
    "# s=Capitalratio(\"YOU are a lazy SLOTH who is gonna DIE in PAIN.\")     #non- processed data\n",
    "# print(s,\"=\",round(s,3))\n",
    "\n",
    "# ss1=sentencelenratio(\"you are idiot. please get a life. stop bothering people.\",50)\n",
    "# ss2=sentencelenratio(\"you are idiot. please get a life. stop bothering people.\",25)    #on processed data\n",
    "# print(ss1)\n",
    "# print(ss2)\n",
    "\n",
    "# f1=badcount(\"Do not give those stupid explaination of yours\", bad_list)   #on processed data\n",
    "# print(f1[0])   #can be ignored\n",
    "# print(f1[1])\n",
    "\n",
    "# f2=positivecount(\"Do not give those stupid explaination of yours\",good_list)      #on processed data\n",
    "# print(f1[0]/(f2[0]+1))      #assumed that there is atleast one positive word i.e normalized for cases where f2[0]=0\n",
    "# #uppercase_count_col.append(round(val, 2))\n",
    "\n",
    "# # on processed-data\n",
    "\n",
    "\n",
    "# sc=badcount('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',bad_list)\n",
    "# print(sc[0])\n",
    "# print(sc[1])\n",
    "# sc2=positivecount('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',good_list)\n",
    "# print(sc2[0])\n",
    "# print(sc2[1])\n",
    "# print(sc[0]/(sc2[0]+1))\n",
    "# sc=sentencelenratio('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',60)\n",
    "# print(sc)\n",
    "# sc=sentencelenratio('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',25)\n",
    "# print(sc)\n",
    "# sc=capitalratio('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!')\n",
    "# print(sc)\n",
    "# send_list=[0,1.5,.6,0,1,.4,.6,.3,.15,.8,.45]\n",
    "# sc=OffensivenessScore('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',send_list,bad_list,good_list)\n",
    "# print(sc)\n",
    "# sc=lexicalscore2('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',bad_list)\n",
    "# print(sc)\n",
    "# sc=lexicalscore3('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',bad_list)\n",
    "# print(sc)\n",
    "sc=lexicalscore2('The only \"other\" relevant fact is that HE IS DESTROYING AMERICA!',bad_list)\n",
    "print(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
